
[2025-08-17 14:29:44]
âœ… requests already installed.
----------------------------------------

[2025-08-17 14:29:44]
ðŸ“¦ Installing beautifulsoup4 ...
----------------------------------------

[2025-08-17 14:29:45]
ðŸ“œ Executing Code:
import requests
from bs4 import BeautifulSoup

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    tables = soup.find_all("table", {"class": "wikitable"})
    summary = ""
    if tables:
        # The main table is usually the first one with the class 'wikitable sortable'
        main_table = soup.find("table", {"class": "wikitable sortable"})
        if main_table:
            headers = [th.text.strip() for th in main_table.find_all("th")]
            summary = f"Found a table with headers: {headers}. It seems to contain the list of highest-grossing films."
        else:
            summary = f'Found {len(tables)} wikitable(s). The first one has headers: {[th.text.strip() for th in tables[0].find_all("th")]}'
    else:
        summary = "No wikitable found on the page."

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(summary + "\n")
else:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Failed to fetch URL. Status code: {response.status_code}\n")

----------------------------------------

[2025-08-17 14:29:47]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:29:55]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:29:55]
âœ… lxml already installed.
----------------------------------------

[2025-08-17 14:29:55]
ðŸ“œ Executing Code:
import pandas as pd
import re

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

try:
    tables = pd.read_html(url, attrs={"class": "wikitable sortable"})
    df = tables[0]
    df.to_csv(
        "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv",
        index=False,
    )

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(
            "Successfully scraped and saved the table of highest-grossing films to highest_grossing_films.csv\n"
        )
except Exception as e:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Error scraping or saving data: {e}\n")

----------------------------------------

[2025-08-17 14:29:58]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:30:05]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:30:05]
âœ… lxml already installed.
----------------------------------------

[2025-08-17 14:30:05]
ðŸ“¦ Installing html5lib ...
----------------------------------------

[2025-08-17 14:30:06]
ðŸ“œ Executing Code:
import pandas as pd
import re

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

try:
    tables = pd.read_html(url, attrs={"class": "wikitable sortable"})
    df = tables[0]
    df.to_csv(
        "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv",
        index=False,
    )

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(
            "Successfully scraped and saved the table of highest-grossing films to highest_grossing_films.csv\n"
        )
except Exception as e:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Error scraping or saving data: {e}\n")

----------------------------------------

[2025-08-17 14:30:12]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:30:26]
âœ… requests already installed.
----------------------------------------

[2025-08-17 14:30:26]
ðŸ“¦ Installing beautifulsoup4 ...
----------------------------------------

[2025-08-17 14:30:27]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:30:27]
ðŸ“œ Executing Code:
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"
response = requests.get(url)

if response.status_code == 200:
    soup = BeautifulSoup(response.content, "html.parser")
    # Find the correct table which contains "Highest-grossing films"
    table = soup.find("table", {"class": "wikitable sortable"})

    if table:
        headers = [th.text.strip() for th in table.find_all("th")]
        rows = []
        for tr in table.find_all("tr")[1:]:  # Skip header row
            cells = [td.text.strip() for td in tr.find_all("td")]
            if len(cells) == len(headers):
                rows.append(cells)

        df = pd.DataFrame(rows, columns=headers)
        df.to_csv(
            "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv",
            index=False,
        )

        with open(
            "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a"
        ) as f:
            f.write(
                "Successfully used BeautifulSoup to parse and save the table to highest_grossing_films.csv\n"
            )
    else:
        with open(
            "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a"
        ) as f:
            f.write(
                "Could not find the wikitable sortable table using BeautifulSoup.\n"
            )
else:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Failed to fetch URL. Status code: {response.status_code}\n")

----------------------------------------

[2025-08-17 14:30:29]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:30:40]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:30:40]
âœ… requests already installed.
----------------------------------------

[2025-08-17 14:30:40]
âœ… lxml already installed.
----------------------------------------

[2025-08-17 14:30:40]
ðŸ“œ Executing Code:
import pandas as pd
import requests

url = "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36"
}

try:
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raise an exception for bad status codes
    tables = pd.read_html(response.text)
    correct_table_found = False
    for df in tables:
        # The correct table should have these columns
        if all(
            col in df.columns for col in ["Rank", "Title", "Worldwide gross", "Year"]
        ):
            df.to_csv(
                "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv",
                index=False,
            )
            with open(
                "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a"
            ) as f:
                f.write(
                    "Successfully found and saved the correct table to highest_grossing_films.csv\n"
                )
            correct_table_found = True
            break
    if not correct_table_found:
        with open(
            "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a"
        ) as f:
            f.write("Could not find the correct table in the page.\n")

except Exception as e:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"An error occurred: {e}\n")

----------------------------------------

[2025-08-17 14:30:42]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:31:22]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:31:22]
âœ… numpy already installed.
----------------------------------------

[2025-08-17 14:31:22]
âœ… matplotlib already installed.
----------------------------------------

[2025-08-17 14:31:22]
âœ… seaborn already installed.
----------------------------------------

[2025-08-17 14:31:22]
ðŸ“œ Executing Code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import json
import re

# Load the data
file_path = "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv"
try:
    df = pd.read_csv(file_path)

    # --- Data Cleaning ---
    # The first table from Wikipedia has a multi-level header, which pandas might read incorrectly.
    # Let's clean up the column names if necessary.
    df.columns = [
        re.sub(r"\.\d+$", "", col) for col in df.columns
    ]  # Remove suffixes like .1, .2

    # Clean 'Worldwide gross' column
    df["Worldwide gross"] = (
        df["Worldwide gross"]
        .astype(str)
        .str.replace(r"\$|,|\[[^\]]*\]", "", regex=True)
    )
    df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

    # Clean 'Year' column
    df["Year"] = pd.to_numeric(df["Year"], errors="coerce")

    # Clean 'Rank' and 'Peak' columns
    df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
    df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

    df.dropna(subset=["Worldwide gross", "Year", "Rank", "Peak"], inplace=True)
    df["Year"] = df["Year"].astype(int)

    # --- Question 1: How many $2 bn movies were released before 2000? ---
    movies_2bn_before_2000 = df[
        (df["Worldwide gross"] >= 2_000_000_000) & (df["Year"] < 2000)
    ]
    answer1 = len(movies_2bn_before_2000)

    # --- Question 2: Which is the earliest film that grossed over $1.5 bn? ---
    movies_over_1_5bn = df[df["Worldwide gross"] >= 1_500_000_000]
    earliest_film = movies_over_1_5bn.sort_values(by="Year", ascending=True).iloc[0]
    answer2 = earliest_film["Title"]

    # --- Question 3: What's the correlation between the Rank and Peak? ---
    correlation = df["Rank"].corr(df["Peak"])
    answer3 = correlation

    # --- Question 4: Draw a scatterplot of Rank and Peak ---
    plt.figure(figsize=(8, 6))
    sns.regplot(
        x="Rank",
        y="Peak",
        data=df,
        scatter_kws={"alpha": 0.6},
        line_kws={"color": "red", "linestyle": "--"},
    )
    plt.title("Rank vs. Peak of Highest-Grossing Films")
    plt.xlabel("Current Rank")
    plt.ylabel("Peak Rank")
    plt.grid(True)

    buf = io.BytesIO()
    plt.savefig(buf, format="png", bbox_inches="tight")
    buf.seek(0)
    image_base64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    answer4 = f"data:image/png;base64,{image_base64}"
    plt.close()

    # --- Final Output ---
    final_answers = [answer1, answer2, answer3, answer4]

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/result.json", "w") as f:
        json.dump(final_answers, f)

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write("Successfully processed data and generated all answers.\n")

except Exception as e:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Error processing data: {e}\n")

----------------------------------------

[2025-08-17 14:31:26]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:32:11]
âœ… pandas already installed.
----------------------------------------

[2025-08-17 14:32:11]
âœ… numpy already installed.
----------------------------------------

[2025-08-17 14:32:11]
âœ… matplotlib already installed.
----------------------------------------

[2025-08-17 14:32:11]
âœ… seaborn already installed.
----------------------------------------

[2025-08-17 14:32:11]
ðŸ“œ Executing Code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import json
import re

# Load the data
file_path = "uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/highest_grossing_films.csv"
try:
    df = pd.read_csv(file_path)

    # --- Corrected Data Cleaning ---
    # Using a more robust regex to remove all non-digit characters to ensure conversion to numeric works correctly.
    df["Worldwide gross"] = (
        df["Worldwide gross"].astype(str).str.replace(r"[^\d]", "", regex=True)
    )
    df["Worldwide gross"] = pd.to_numeric(df["Worldwide gross"], errors="coerce")

    df["Year"] = pd.to_numeric(df["Year"], errors="coerce")
    df["Rank"] = pd.to_numeric(df["Rank"], errors="coerce")
    df["Peak"] = pd.to_numeric(df["Peak"], errors="coerce")

    df.dropna(subset=["Worldwide gross", "Year", "Rank", "Peak"], inplace=True)
    df["Year"] = df["Year"].astype(int)

    # --- Question 1: How many $2 bn movies were released before 2000? ---
    movies_2bn_before_2000 = df[
        (df["Worldwide gross"] >= 2_000_000_000) & (df["Year"] < 2000)
    ]
    answer1 = len(movies_2bn_before_2000)

    # --- Question 2: Which is the earliest film that grossed over $1.5 bn? ---
    movies_over_1_5bn = df[df["Worldwide gross"] >= 1_500_000_000]
    earliest_film = movies_over_1_5bn.sort_values(by="Year", ascending=True).iloc[0]
    answer2 = earliest_film["Title"]

    # --- Question 3: What's the correlation between the Rank and Peak? ---
    correlation = df["Rank"].corr(df["Peak"])
    answer3 = correlation

    # --- Question 4: Draw a scatterplot of Rank and Peak ---
    plt.figure(figsize=(8, 6))
    sns.regplot(
        x="Rank",
        y="Peak",
        data=df,
        scatter_kws={"alpha": 0.6},
        line_kws={"color": "red", "linestyle": "--"},
    )
    plt.title("Rank vs. Peak of Highest-Grossing Films")
    plt.xlabel("Current Rank")
    plt.ylabel("Peak Rank")
    plt.grid(True)

    buf = io.BytesIO()
    plt.savefig(buf, format="png", bbox_inches="tight")
    buf.seek(0)
    image_base64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    answer4 = f"data:image/png;base64,{image_base64}"
    plt.close()

    # --- Final Output ---
    final_answers = [int(answer1), str(answer2), float(answer3), str(answer4)]

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/result.json", "w") as f:
        json.dump(final_answers, f)

    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write("Corrected data processing. Generated all answers.\n")

except Exception as e:
    with open("uploads/a44c8d0a-afd8-444c-9162-619dea3d24e4/metadata.txt", "a") as f:
        f.write(f"Error during corrected data processing: {e}\n")

----------------------------------------

[2025-08-17 14:32:14]
âœ… Code executed successfully:

----------------------------------------

[2025-08-17 14:32:26]
ðŸ“œ Executing Code:

----------------------------------------

[2025-08-17 14:32:26]
âœ… Code executed successfully:

----------------------------------------
